nohup: ignoring input
2022-04-22 14:44:10.960222: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-04-22 14:44:10.960257: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
environment loaded !
Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Logging to ./logs/GymEnvWithRecoWithDN_2022-04-22_14-44_learning_rate_0/PPO_1
2050-08-01_7: env.nb_time_step = 2017, reward : 1.000
2050-01-10_0: env.nb_time_step = 569, reward : 0.531
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.5     |
|    ep_rew_mean     | 0.266    |
| time/              |          |
|    fps             | 36       |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 256      |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 97.5       |
|    ep_rew_mean          | 0.266      |
| time/                   |            |
|    fps                  | 57         |
|    iterations           | 2          |
|    time_elapsed         | 8          |
|    total_timesteps      | 512        |
| train/                  |            |
|    approx_kl            | 0.10453945 |
|    clip_fraction        | 0.62       |
|    clip_range           | 0.2        |
|    entropy_loss         | -69.5      |
|    explained_variance   | -5.76      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.128     |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0892    |
|    std                  | 1          |
|    value_loss           | 0.0339     |
----------------------------------------
/home/boguslawskieva/miniconda3/envs/baselines_dev/lib/python3.8/site-packages/grid2op/Environment/BaseEnv.py:2335: RuntimeWarning: invalid value encountered in true_divide
  modif_storage = new_act_storage * total_storage / sum_this_step
2050-01-10_0: env.nb_time_step = 1006, reward : 0.706
2050-08-01_7: env.nb_time_step = 2017, reward : 1.000
Traceback (most recent call last):
  File "agent_experiments.py", line 118, in <module>
    agents = iter_hyperparameters(env_train, train_args, name, var_to_test, values_to_test)
  File "/home/boguslawskieva/L2RPN-WCCI-Baselines/RL/utils.py", line 158, in iter_hyperparameters
    ret_agents.append((train_args["name"], train_agent(env, train_args, max_iter)))
  File "/home/boguslawskieva/L2RPN-WCCI-Baselines/RL/utils.py", line 110, in train_agent
    return train(env, **train_args)
  File "/home/boguslawskieva/L2RPN-WCCI-Baselines/RL/l2rpn_baselines/PPO_SB3/train.py", line 282, in train
    agent.nn_model.learn(total_timesteps=iterations,
  File "/home/boguslawskieva/miniconda3/envs/baselines_dev/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py", line 304, in learn
    return super(PPO, self).learn(
  File "/home/boguslawskieva/miniconda3/envs/baselines_dev/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 250, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/boguslawskieva/miniconda3/envs/baselines_dev/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 169, in collect_rollouts
    actions, values, log_probs = self.policy(obs_tensor)
  File "/home/boguslawskieva/miniconda3/envs/baselines_dev/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/boguslawskieva/miniconda3/envs/baselines_dev/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 592, in forward
    distribution = self._get_action_dist_from_latent(latent_pi)
  File "/home/boguslawskieva/miniconda3/envs/baselines_dev/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 607, in _get_action_dist_from_latent
    return self.action_dist.proba_distribution(mean_actions, self.log_std)
  File "/home/boguslawskieva/miniconda3/envs/baselines_dev/lib/python3.8/site-packages/stable_baselines3/common/distributions.py", line 152, in proba_distribution
    self.distribution = Normal(mean_actions, action_std)
  File "/home/boguslawskieva/miniconda3/envs/baselines_dev/lib/python3.8/site-packages/torch/distributions/normal.py", line 50, in __init__
    super(Normal, self).__init__(batch_shape, validate_args=validate_args)
  File "/home/boguslawskieva/miniconda3/envs/baselines_dev/lib/python3.8/site-packages/torch/distributions/distribution.py", line 55, in __init__
    raise ValueError(
ValueError: Expected parameter loc (Tensor of shape (1, 49)) of distribution Normal(loc: torch.Size([1, 49]), scale: torch.Size([1, 49])) to satisfy the constraint Real(), but found invalid values:
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
         nan]], device='cuda:0')
