{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from CustomGymEnv import CustomGymEnv\n",
    "import torch\n",
    "from grid2op.Parameters import Parameters\n",
    "from examples.ppo_stable_baselines.B_train_agent import CustomReward\n",
    "from lightsim2grid import LightSimBackend\n",
    "from grid2op.Chronics import MultifolderWithCache\n",
    "from grid2op.utils import ScoreL2RPN2020\n",
    "from examples.ppo_stable_baselines.A_prep_env import get_env_seed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2020-2022 RTE (https://www.rte-france.com)\n",
    "# See AUTHORS.txt\n",
    "# This Source Code Form is subject to the terms of the Mozilla Public License, version 2.0.\n",
    "# If a copy of the Mozilla Public License, version 2.0 was not distributed with this file,\n",
    "# you can obtain one at http://mozilla.org/MPL/2.0/.\n",
    "# SPDX-License-Identifier: MPL-2.0\n",
    "# This file is part of L2RPN Baselines, L2RPN Baselines a repository to host baselines for l2rpn competitions.\n",
    "\n",
    "import warnings\n",
    "import copy\n",
    "import os\n",
    "import grid2op\n",
    "import json\n",
    "\n",
    "from grid2op.gym_compat import BoxGymActSpace, BoxGymObsSpace, GymEnv\n",
    "\n",
    "from l2rpn_baselines.PPO_SB3.utils import SB3Agent\n",
    "\n",
    "try:\n",
    "    from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "    from stable_baselines3 import PPO\n",
    "    from stable_baselines3.ppo import MlpPolicy\n",
    "    _CAN_USE_STABLE_BASELINE = True\n",
    "except ImportError:\n",
    "    _CAN_USE_STABLE_BASELINE = False\n",
    "    class MlpPolicy(object):\n",
    "        \"\"\"\n",
    "        Do not use, this class is a template when stable baselines3 is not installed.\n",
    "        \n",
    "        It represents `from stable_baselines3.ppo import MlpPolicy`\n",
    "        \"\"\"\n",
    "    \n",
    "from l2rpn_baselines.PPO_SB3.utils import (default_obs_attr_to_keep, \n",
    "                                           default_act_attr_to_keep,\n",
    "                                           remove_non_usable_attr,\n",
    "                                           save_used_attribute)\n",
    "\n",
    "\n",
    "def get_agent(env,\n",
    "          name=\"PPO_SB3\",\n",
    "          iterations=1,\n",
    "          save_path=None,\n",
    "          load_path=None,\n",
    "          net_arch=None,\n",
    "          logs_dir=None,\n",
    "          learning_rate=3e-4,\n",
    "          save_every_xxx_steps=None,\n",
    "          model_policy=MlpPolicy,\n",
    "          obs_attr_to_keep=copy.deepcopy(default_obs_attr_to_keep),\n",
    "          obs_space_kwargs=None,\n",
    "          act_attr_to_keep=copy.deepcopy(default_act_attr_to_keep),\n",
    "          act_space_kwargs=None,\n",
    "          policy_kwargs=None,\n",
    "          normalize_obs=False,\n",
    "          normalize_act=False,\n",
    "          gymenv_class=GymEnv,\n",
    "          gymenv_kwargs=None,\n",
    "          verbose=True,\n",
    "          seed=None,  # TODO\n",
    "          eval_env=None,  # TODO\n",
    "          **kwargs):\n",
    "    \n",
    "    if not _CAN_USE_STABLE_BASELINE:\n",
    "        raise ImportError(\"Cannot use this function as stable baselines3 is not installed\")\n",
    "    \n",
    "    # keep only usable attributes (if default is used)\n",
    "    act_attr_to_keep = remove_non_usable_attr(env, act_attr_to_keep)\n",
    "    \n",
    "    # save the attributes kept\n",
    "    if save_path is not None:\n",
    "        my_path = os.path.join(save_path, name)\n",
    "    save_used_attribute(save_path, name, obs_attr_to_keep, act_attr_to_keep)\n",
    "\n",
    "    # define the gym environment from the grid2op env\n",
    "    if gymenv_kwargs is None:\n",
    "        gymenv_kwargs = {}\n",
    "    env_gym = gymenv_class(env, **gymenv_kwargs)\n",
    "    env_gym.observation_space.close()\n",
    "    if obs_space_kwargs is None:\n",
    "        obs_space_kwargs = {}\n",
    "    env_gym.observation_space = BoxGymObsSpace(env.observation_space,\n",
    "                                               attr_to_keep=obs_attr_to_keep,\n",
    "                                               **obs_space_kwargs)\n",
    "    env_gym.action_space.close()\n",
    "    if act_space_kwargs is None:\n",
    "        act_space_kwargs = {}\n",
    "    env_gym.action_space = BoxGymActSpace(env.action_space,\n",
    "                                          attr_to_keep=act_attr_to_keep,\n",
    "                                          **act_space_kwargs)\n",
    "\n",
    "    if normalize_act:\n",
    "        if save_path is not None:\n",
    "            with open(os.path.join(my_path, \".normalize_act\"), encoding=\"utf-8\", \n",
    "                      mode=\"w\") as f:\n",
    "                f.write(\"I have encoded the action space !\\n DO NOT MODIFY !\")\n",
    "        for attr_nm in act_attr_to_keep:\n",
    "            if ((\"multiply\" in act_space_kwargs and attr_nm in act_space_kwargs[\"multiply\"]) or \n",
    "                (\"add\" in act_space_kwargs and attr_nm in act_space_kwargs[\"add\"]) \n",
    "               ):\n",
    "                # attribute is scaled elsewhere\n",
    "                continue\n",
    "            env_gym.action_space.normalize_attr(attr_nm)\n",
    "\n",
    "    if normalize_obs:\n",
    "        if save_path is not None:\n",
    "            with open(os.path.join(my_path, \".normalize_obs\"), encoding=\"utf-8\", \n",
    "                      mode=\"w\") as f:\n",
    "                f.write(\"I have encoded the observation space !\\n DO NOT MODIFY !\")\n",
    "        for attr_nm in obs_attr_to_keep:\n",
    "            if ((\"divide\" in obs_space_kwargs and attr_nm in obs_space_kwargs[\"divide\"]) or \n",
    "                (\"subtract\" in obs_space_kwargs and attr_nm in obs_space_kwargs[\"subtract\"]) \n",
    "               ):\n",
    "                # attribute is scaled elsewhere\n",
    "                continue\n",
    "            env_gym.observation_space.normalize_attr(attr_nm)\n",
    "    \n",
    "    # Save a checkpoint every \"save_every_xxx_steps\" steps\n",
    "    checkpoint_callback = None\n",
    "    if save_every_xxx_steps is not None:\n",
    "        if save_path is None:\n",
    "            warnings.warn(\"save_every_xxx_steps is set, but no path are \"\n",
    "                          \"set to save the model (save_path is None). No model \"\n",
    "                          \"will be saved.\")\n",
    "        else:\n",
    "            checkpoint_callback = CheckpointCallback(save_freq=save_every_xxx_steps,\n",
    "                                                     save_path=my_path,\n",
    "                                                     name_prefix=name)\n",
    "\n",
    "    # define the policy\n",
    "    if load_path is None:\n",
    "        if policy_kwargs is None:\n",
    "            policy_kwargs = {}\n",
    "        if net_arch is not None:\n",
    "            policy_kwargs[\"net_arch\"] = net_arch\n",
    "        if logs_dir is not None:\n",
    "            if not os.path.exists(logs_dir):\n",
    "                os.mkdir(logs_dir)\n",
    "            this_logs_dir = os.path.join(logs_dir, name)\n",
    "        else:\n",
    "            this_logs_dir = None\n",
    "                \n",
    "        nn_kwargs = {\n",
    "            \"policy\": model_policy,\n",
    "            \"env\": env_gym,\n",
    "            \"verbose\": verbose,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"tensorboard_log\": this_logs_dir,\n",
    "            \"policy_kwargs\": policy_kwargs,\n",
    "            **kwargs\n",
    "        }\n",
    "        agent = SB3Agent(env.action_space,\n",
    "                         env_gym.action_space,\n",
    "                         env_gym.observation_space,\n",
    "                         nn_kwargs=nn_kwargs,\n",
    "        )\n",
    "    else:        \n",
    "        agent = SB3Agent(env.action_space,\n",
    "                         env_gym.action_space,\n",
    "                         env_gym.observation_space,\n",
    "                         nn_path=os.path.join(load_path, name)\n",
    "        )\n",
    "    return agent, env_gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"l2rpn_wcci_2022_dev\"\n",
    "env_name_train = '_'.join([ENV_NAME, \"train\"])\n",
    "save_path = \"./student\"\n",
    "name = \"CustomGymEnv\"\n",
    "gymenv_class = CustomGymEnv\n",
    "SCOREUSED = ScoreL2RPN2020\n",
    "\n",
    "train_args = {}\n",
    "\n",
    "# Utility parameters PPO\n",
    "train_args[\"logs_dir\"] = \"./logs\"\n",
    "train_args[\"save_path\"] = save_path\n",
    "train_args[\"name\"] = name\n",
    "train_args[\"verbose\"] = 1\n",
    "train_args[\"gymenv_class\"] = gymenv_class\n",
    "train_args[\"device\"] = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_args[\"obs_attr_to_keep\"] = [\"month\", \"day_of_week\", \"hour_of_day\", \"minute_of_hour\",\n",
    "                                  \"gen_p\", \"load_p\", \n",
    "                                  \"p_or\", \"rho\", \"timestep_overflow\", \"line_status\",\n",
    "                                  # dispatch part of the observation\n",
    "                                  \"actual_dispatch\", \"target_dispatch\",\n",
    "                                  # storage part of the observation\n",
    "                                  \"storage_charge\", \"storage_power\",\n",
    "                                  # curtailment part of the observation\n",
    "                                  \"curtailment\", \"curtailment_limit\",  \"gen_p_before_curtail\",\n",
    "                                  ]\n",
    "train_args[\"act_attr_to_keep\"] = [\"set_storage\", \"curtail\"]\n",
    "train_args[\"iterations\"] = 700_000\n",
    "train_args[\"learning_rate\"] = 1e-4\n",
    "train_args[\"net_arch\"] = [300, 300, 300]\n",
    "train_args[\"gamma\"] = 0.999\n",
    "train_args[\"gymenv_kwargs\"] = {\"safe_max_rho\": 0.95}\n",
    "train_args[\"normalize_act\"] = True\n",
    "train_args[\"normalize_obs\"] = True\n",
    "\n",
    "train_args[\"save_every_xxx_steps\"] = min(train_args[\"iterations\"] // 10, 100_000)\n",
    "\n",
    "train_args[\"n_steps\"] = 16\n",
    "train_args[\"batch_size\"] = 16\n",
    "\n",
    "p = Parameters()\n",
    "p.LIMIT_INFEASIBLE_CURTAILMENT_STORAGE_ACTION = True\n",
    "\n",
    "env = grid2op.make(ENV_NAME,\n",
    "                   reward_class=CustomReward,\n",
    "                   backend=LightSimBackend(),\n",
    "                   chronics_class=MultifolderWithCache,\n",
    "                   param=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../preprocess_obs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "  obs_space_kwargs = json.load(f)\n",
    "with open(\"../preprocess_act.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "  act_space_kwargs = json.load(f)\n",
    "\n",
    "student, gym_env = get_agent(env,\n",
    "          obs_space_kwargs=obs_space_kwargs,\n",
    "          act_space_kwargs=act_space_kwargs,\n",
    "          **train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"expert_data.npz\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-train the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class ExpertDataSet(Dataset):\n",
    "  def __init__(self, expert_observations, expert_actions):\n",
    "      self.observations = expert_observations\n",
    "      self.actions = expert_actions\n",
    "      \n",
    "  def __getitem__(self, index):\n",
    "      return (self.observations[index], self.actions[index])\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_acts(actions):\n",
    "  converted_acts = []\n",
    "  for act in actions:\n",
    "    low = gym_env.action_space.dict_properties[\"curtail\"][0]\n",
    "    high = gym_env.action_space.dict_properties[\"curtail\"][1]\n",
    "\n",
    "    curtail = act.curtail.copy()[env.gen_renewable]\n",
    "    curtail = curtail / (high - low) + low\n",
    "\n",
    "    low = gym_env.action_space.dict_properties[\"set_storage\"][0]\n",
    "    high = gym_env.action_space.dict_properties[\"set_storage\"][1]\n",
    "\n",
    "    storage = act.storage_p.copy() / act_space_kwargs[\"multiply\"][\"set_storage\"]\n",
    "\n",
    "    converted_acts.append(np.concatenate((curtail, storage)))\n",
    "  return converted_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_obs(observations):\n",
    "  converted_obs = []\n",
    "  g2op_obs = env.reset()\n",
    "  for obs in observations:\n",
    "    g2op_obs.from_vect(obs)\n",
    "    converted_obs.append(gym_env.observation_space.to_gym(g2op_obs))\n",
    "  return converted_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_observations = convert_obs(data.get(\"expert_observations\"))\n",
    "expert_actions      = convert_acts(data.get(\"expert_actions\"))\n",
    "\n",
    "expert_dataset = ExpertDataSet(expert_observations, expert_actions)\n",
    "\n",
    "train_size = int(0.8 * len(expert_dataset))\n",
    "\n",
    "test_size = len(expert_dataset) - train_size\n",
    "\n",
    "train_expert_dataset, test_expert_dataset = random_split(\n",
    "    expert_dataset, [train_size, test_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_expert_dataset:  21\n",
      "train_expert_dataset:  84\n"
     ]
    }
   ],
   "source": [
    "print(\"test_expert_dataset: \", len(test_expert_dataset))\n",
    "print(\"train_expert_dataset: \", len(train_expert_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_agent(\n",
    "    student,\n",
    "    batch_size=64,\n",
    "    epochs=1000,\n",
    "    learning_rate=1e-4,\n",
    "    log_interval=10,\n",
    "    no_cuda=False,\n",
    "    seed=42,\n",
    "    test_batch_size=64,\n",
    "):\n",
    "  use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "  torch.manual_seed(seed)\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "  criterion = nn.MSELoss()\n",
    "\n",
    "  # Extract initial policy\n",
    "  model = student.nn_model.policy.to(device)\n",
    "\n",
    "  def train(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      action, _, _ = model(data)\n",
    "      action_prediction = action.double()\n",
    "\n",
    "      loss = criterion(action_prediction, target)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      if batch_idx % log_interval == 0:\n",
    "        print(\n",
    "          \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "            epoch,\n",
    "            batch_idx * len(data),\n",
    "            len(train_loader.dataset),\n",
    "            100.0 * batch_idx / len(train_loader),\n",
    "            loss.item()\n",
    "          )\n",
    "        )\n",
    "\n",
    "  def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        action, _, _ = model(data)\n",
    "        action_prediction = action.double()\n",
    "        test_loss += criterion(action_prediction, target)\n",
    "    test_loss /= len(test_loader) # Nb batches\n",
    "    print(f\"Test set: Average loss: {test_loss.item():.4f}\")\n",
    "\n",
    "  # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
    "  # and testing\n",
    "  train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_expert_dataset, batch_size=batch_size, shuffle=True\n",
    "  )\n",
    "  test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_expert_dataset, batch_size=test_batch_size, shuffle=True\n",
    "  )\n",
    "\n",
    "  # Define an Optimizer and a learning rate schedule.\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  # Now we are finally ready to train the policy model.\n",
    "  for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer)\n",
    "    test(model, device, test_loader)\n",
    "    optimizer.step()\n",
    "  student.nn_model.policy = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_agent(agent, env_name, nb_scenario, param, verbose):\n",
    "  env_val = grid2op.make(env_name, backend=LightSimBackend(), param=param)\n",
    "  my_score = SCOREUSED(env_val,\n",
    "                        nb_scenario=nb_scenario,\n",
    "                        env_seeds=get_env_seed(env_name)[:nb_scenario],\n",
    "                        agent_seeds=[0 for _ in range(nb_scenario)],\n",
    "                        verbose=verbose,\n",
    "                        nb_process_stats=1)\n",
    "  _, ts_survived, _ = my_score.get(agent)\n",
    "  return np.array(ts_survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval before pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts the evaluation of the agent\n",
      "Start the evaluation of the scores\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47.46666666666667"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = eval_agent(student, \"l2rpn_wcci_2022_dev_val\", 59, p, 1)\n",
    "ts.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/84 (0%)]\tLoss: 589.405251\n",
      "Test set: Average loss: 650.6445\n",
      "Train Epoch: 2 [0/84 (0%)]\tLoss: 691.173479\n",
      "Test set: Average loss: 652.1056\n",
      "Train Epoch: 3 [0/84 (0%)]\tLoss: 734.487176\n",
      "Test set: Average loss: 656.0158\n",
      "Train Epoch: 4 [0/84 (0%)]\tLoss: 637.947154\n",
      "Test set: Average loss: 656.7049\n",
      "Train Epoch: 5 [0/84 (0%)]\tLoss: 707.617758\n",
      "Test set: Average loss: 652.9761\n",
      "Train Epoch: 6 [0/84 (0%)]\tLoss: 651.123751\n",
      "Test set: Average loss: 650.6375\n",
      "Train Epoch: 7 [0/84 (0%)]\tLoss: 555.331774\n",
      "Test set: Average loss: 650.3825\n",
      "Train Epoch: 8 [0/84 (0%)]\tLoss: 699.356967\n",
      "Test set: Average loss: 649.4346\n",
      "Train Epoch: 9 [0/84 (0%)]\tLoss: 718.321014\n",
      "Test set: Average loss: 648.8337\n",
      "Train Epoch: 10 [0/84 (0%)]\tLoss: 626.618979\n",
      "Test set: Average loss: 650.7548\n",
      "Train Epoch: 11 [0/84 (0%)]\tLoss: 612.058830\n",
      "Test set: Average loss: 648.7607\n",
      "Train Epoch: 12 [0/84 (0%)]\tLoss: 608.673830\n",
      "Test set: Average loss: 650.4288\n",
      "Train Epoch: 13 [0/84 (0%)]\tLoss: 686.341498\n",
      "Test set: Average loss: 648.2427\n",
      "Train Epoch: 14 [0/84 (0%)]\tLoss: 632.198833\n",
      "Test set: Average loss: 645.4207\n",
      "Train Epoch: 15 [0/84 (0%)]\tLoss: 645.259169\n",
      "Test set: Average loss: 647.5777\n",
      "Train Epoch: 16 [0/84 (0%)]\tLoss: 665.115547\n",
      "Test set: Average loss: 645.7261\n",
      "Train Epoch: 17 [0/84 (0%)]\tLoss: 684.092383\n",
      "Test set: Average loss: 649.0939\n",
      "Train Epoch: 18 [0/84 (0%)]\tLoss: 677.492181\n",
      "Test set: Average loss: 645.1483\n",
      "Train Epoch: 19 [0/84 (0%)]\tLoss: 533.687899\n",
      "Test set: Average loss: 646.4746\n",
      "Train Epoch: 20 [0/84 (0%)]\tLoss: 690.846188\n",
      "Test set: Average loss: 646.0707\n",
      "Train Epoch: 21 [0/84 (0%)]\tLoss: 644.264303\n",
      "Test set: Average loss: 648.3390\n",
      "Train Epoch: 22 [0/84 (0%)]\tLoss: 724.132158\n",
      "Test set: Average loss: 644.8365\n",
      "Train Epoch: 23 [0/84 (0%)]\tLoss: 683.438892\n",
      "Test set: Average loss: 644.8546\n",
      "Train Epoch: 24 [0/84 (0%)]\tLoss: 659.926536\n",
      "Test set: Average loss: 642.7270\n",
      "Train Epoch: 25 [0/84 (0%)]\tLoss: 728.002745\n",
      "Test set: Average loss: 642.1683\n",
      "Train Epoch: 26 [0/84 (0%)]\tLoss: 618.806895\n",
      "Test set: Average loss: 642.7671\n",
      "Train Epoch: 27 [0/84 (0%)]\tLoss: 630.311730\n",
      "Test set: Average loss: 640.6018\n",
      "Train Epoch: 28 [0/84 (0%)]\tLoss: 621.569377\n",
      "Test set: Average loss: 641.5501\n",
      "Train Epoch: 29 [0/84 (0%)]\tLoss: 638.396406\n",
      "Test set: Average loss: 639.9181\n",
      "Train Epoch: 30 [0/84 (0%)]\tLoss: 608.801179\n",
      "Test set: Average loss: 639.0497\n",
      "Train Epoch: 31 [0/84 (0%)]\tLoss: 652.728271\n",
      "Test set: Average loss: 638.8371\n",
      "Train Epoch: 32 [0/84 (0%)]\tLoss: 719.575016\n",
      "Test set: Average loss: 640.4356\n",
      "Train Epoch: 33 [0/84 (0%)]\tLoss: 656.617273\n",
      "Test set: Average loss: 639.0695\n",
      "Train Epoch: 34 [0/84 (0%)]\tLoss: 641.609837\n",
      "Test set: Average loss: 635.4931\n",
      "Train Epoch: 35 [0/84 (0%)]\tLoss: 712.768268\n",
      "Test set: Average loss: 634.1584\n",
      "Train Epoch: 36 [0/84 (0%)]\tLoss: 771.025359\n",
      "Test set: Average loss: 634.2196\n",
      "Train Epoch: 37 [0/84 (0%)]\tLoss: 576.347653\n",
      "Test set: Average loss: 634.6546\n",
      "Train Epoch: 38 [0/84 (0%)]\tLoss: 582.293756\n",
      "Test set: Average loss: 636.7889\n",
      "Train Epoch: 39 [0/84 (0%)]\tLoss: 603.717308\n",
      "Test set: Average loss: 635.9103\n",
      "Train Epoch: 40 [0/84 (0%)]\tLoss: 653.425879\n",
      "Test set: Average loss: 636.0990\n",
      "Train Epoch: 41 [0/84 (0%)]\tLoss: 564.177186\n",
      "Test set: Average loss: 635.4055\n",
      "Train Epoch: 42 [0/84 (0%)]\tLoss: 521.780364\n",
      "Test set: Average loss: 633.6056\n",
      "Train Epoch: 43 [0/84 (0%)]\tLoss: 615.459874\n",
      "Test set: Average loss: 631.8802\n",
      "Train Epoch: 44 [0/84 (0%)]\tLoss: 528.110161\n",
      "Test set: Average loss: 634.4073\n",
      "Train Epoch: 45 [0/84 (0%)]\tLoss: 633.268306\n",
      "Test set: Average loss: 629.2640\n",
      "Train Epoch: 46 [0/84 (0%)]\tLoss: 627.085280\n",
      "Test set: Average loss: 632.4853\n",
      "Train Epoch: 47 [0/84 (0%)]\tLoss: 611.057259\n",
      "Test set: Average loss: 631.6629\n",
      "Train Epoch: 48 [0/84 (0%)]\tLoss: 667.720093\n",
      "Test set: Average loss: 629.9386\n",
      "Train Epoch: 49 [0/84 (0%)]\tLoss: 596.688883\n",
      "Test set: Average loss: 628.8448\n",
      "Train Epoch: 50 [0/84 (0%)]\tLoss: 566.232400\n",
      "Test set: Average loss: 626.7875\n",
      "Train Epoch: 51 [0/84 (0%)]\tLoss: 696.112469\n",
      "Test set: Average loss: 627.7384\n",
      "Train Epoch: 52 [0/84 (0%)]\tLoss: 502.132386\n",
      "Test set: Average loss: 625.7938\n",
      "Train Epoch: 53 [0/84 (0%)]\tLoss: 686.400403\n",
      "Test set: Average loss: 627.4871\n",
      "Train Epoch: 54 [0/84 (0%)]\tLoss: 604.896699\n",
      "Test set: Average loss: 624.5272\n",
      "Train Epoch: 55 [0/84 (0%)]\tLoss: 585.760443\n",
      "Test set: Average loss: 623.8783\n",
      "Train Epoch: 56 [0/84 (0%)]\tLoss: 613.375063\n",
      "Test set: Average loss: 625.0261\n",
      "Train Epoch: 57 [0/84 (0%)]\tLoss: 657.581660\n",
      "Test set: Average loss: 623.1277\n",
      "Train Epoch: 58 [0/84 (0%)]\tLoss: 652.494915\n",
      "Test set: Average loss: 623.3998\n",
      "Train Epoch: 59 [0/84 (0%)]\tLoss: 612.387393\n",
      "Test set: Average loss: 619.5642\n",
      "Train Epoch: 60 [0/84 (0%)]\tLoss: 543.751538\n",
      "Test set: Average loss: 621.0577\n",
      "Train Epoch: 61 [0/84 (0%)]\tLoss: 578.360385\n",
      "Test set: Average loss: 621.7610\n",
      "Train Epoch: 62 [0/84 (0%)]\tLoss: 666.420833\n",
      "Test set: Average loss: 621.2982\n",
      "Train Epoch: 63 [0/84 (0%)]\tLoss: 617.063254\n",
      "Test set: Average loss: 620.5241\n",
      "Train Epoch: 64 [0/84 (0%)]\tLoss: 648.456320\n",
      "Test set: Average loss: 619.9082\n",
      "Train Epoch: 65 [0/84 (0%)]\tLoss: 647.787878\n",
      "Test set: Average loss: 619.1264\n",
      "Train Epoch: 66 [0/84 (0%)]\tLoss: 589.291141\n",
      "Test set: Average loss: 618.4974\n",
      "Train Epoch: 67 [0/84 (0%)]\tLoss: 669.317060\n",
      "Test set: Average loss: 617.4347\n",
      "Train Epoch: 68 [0/84 (0%)]\tLoss: 514.167626\n",
      "Test set: Average loss: 618.5534\n",
      "Train Epoch: 69 [0/84 (0%)]\tLoss: 571.488530\n",
      "Test set: Average loss: 616.1279\n",
      "Train Epoch: 70 [0/84 (0%)]\tLoss: 689.755250\n",
      "Test set: Average loss: 615.4427\n",
      "Train Epoch: 71 [0/84 (0%)]\tLoss: 586.763162\n",
      "Test set: Average loss: 616.0250\n",
      "Train Epoch: 72 [0/84 (0%)]\tLoss: 720.171582\n",
      "Test set: Average loss: 613.8835\n",
      "Train Epoch: 73 [0/84 (0%)]\tLoss: 584.992849\n",
      "Test set: Average loss: 614.4379\n",
      "Train Epoch: 74 [0/84 (0%)]\tLoss: 641.446702\n",
      "Test set: Average loss: 612.4820\n",
      "Train Epoch: 75 [0/84 (0%)]\tLoss: 614.164030\n",
      "Test set: Average loss: 614.1004\n",
      "Train Epoch: 76 [0/84 (0%)]\tLoss: 608.800655\n",
      "Test set: Average loss: 611.5277\n",
      "Train Epoch: 77 [0/84 (0%)]\tLoss: 627.817465\n",
      "Test set: Average loss: 611.6298\n",
      "Train Epoch: 78 [0/84 (0%)]\tLoss: 667.226135\n",
      "Test set: Average loss: 612.7920\n",
      "Train Epoch: 79 [0/84 (0%)]\tLoss: 658.757985\n",
      "Test set: Average loss: 610.3287\n",
      "Train Epoch: 80 [0/84 (0%)]\tLoss: 606.882245\n",
      "Test set: Average loss: 610.5372\n",
      "Train Epoch: 81 [0/84 (0%)]\tLoss: 554.578715\n",
      "Test set: Average loss: 611.4071\n",
      "Train Epoch: 82 [0/84 (0%)]\tLoss: 634.988747\n",
      "Test set: Average loss: 612.3316\n",
      "Train Epoch: 83 [0/84 (0%)]\tLoss: 618.180010\n",
      "Test set: Average loss: 607.7484\n",
      "Train Epoch: 84 [0/84 (0%)]\tLoss: 607.364844\n",
      "Test set: Average loss: 611.5050\n",
      "Train Epoch: 85 [0/84 (0%)]\tLoss: 589.878305\n",
      "Test set: Average loss: 609.9290\n",
      "Train Epoch: 86 [0/84 (0%)]\tLoss: 622.423494\n",
      "Test set: Average loss: 608.8380\n",
      "Train Epoch: 87 [0/84 (0%)]\tLoss: 653.449863\n",
      "Test set: Average loss: 607.2952\n",
      "Train Epoch: 88 [0/84 (0%)]\tLoss: 615.300385\n",
      "Test set: Average loss: 606.8611\n",
      "Train Epoch: 89 [0/84 (0%)]\tLoss: 548.888398\n",
      "Test set: Average loss: 603.6641\n",
      "Train Epoch: 90 [0/84 (0%)]\tLoss: 598.984839\n",
      "Test set: Average loss: 607.1014\n",
      "Train Epoch: 91 [0/84 (0%)]\tLoss: 553.167663\n",
      "Test set: Average loss: 604.0142\n",
      "Train Epoch: 92 [0/84 (0%)]\tLoss: 642.395881\n",
      "Test set: Average loss: 603.8208\n",
      "Train Epoch: 93 [0/84 (0%)]\tLoss: 575.501624\n",
      "Test set: Average loss: 602.6618\n",
      "Train Epoch: 94 [0/84 (0%)]\tLoss: 601.790082\n",
      "Test set: Average loss: 602.2733\n",
      "Train Epoch: 95 [0/84 (0%)]\tLoss: 660.238612\n",
      "Test set: Average loss: 600.9896\n",
      "Train Epoch: 96 [0/84 (0%)]\tLoss: 656.161953\n",
      "Test set: Average loss: 602.5176\n",
      "Train Epoch: 97 [0/84 (0%)]\tLoss: 624.393666\n",
      "Test set: Average loss: 601.2246\n",
      "Train Epoch: 98 [0/84 (0%)]\tLoss: 579.289262\n",
      "Test set: Average loss: 600.9700\n",
      "Train Epoch: 99 [0/84 (0%)]\tLoss: 571.507851\n",
      "Test set: Average loss: 601.1719\n",
      "Train Epoch: 100 [0/84 (0%)]\tLoss: 577.583217\n",
      "Test set: Average loss: 597.8152\n"
     ]
    }
   ],
   "source": [
    "pretrain_agent(student, batch_size=64, epochs=100, log_interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval after pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts the evaluation of the agent\n",
      "Start the evaluation of the scores\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47.2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = eval_agent(student, \"l2rpn_wcci_2022_dev_val\", 59, p, 1)\n",
    "ts.mean()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25edc466125f4cec8abda921974b846defd6af997a9da267187bfa098e2439de"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('L2RPN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
