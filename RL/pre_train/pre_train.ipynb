{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from CustomGymEnv import CustomGymEnv\n",
    "import torch\n",
    "from grid2op.Parameters import Parameters\n",
    "from examples.ppo_stable_baselines.B_train_agent import CustomReward\n",
    "from lightsim2grid import LightSimBackend\n",
    "from grid2op.Chronics import MultifolderWithCache\n",
    "from grid2op.utils import ScoreL2RPN2020\n",
    "from examples.ppo_stable_baselines.A_prep_env import get_env_seed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2020-2022 RTE (https://www.rte-france.com)\n",
    "# See AUTHORS.txt\n",
    "# This Source Code Form is subject to the terms of the Mozilla Public License, version 2.0.\n",
    "# If a copy of the Mozilla Public License, version 2.0 was not distributed with this file,\n",
    "# you can obtain one at http://mozilla.org/MPL/2.0/.\n",
    "# SPDX-License-Identifier: MPL-2.0\n",
    "# This file is part of L2RPN Baselines, L2RPN Baselines a repository to host baselines for l2rpn competitions.\n",
    "\n",
    "import warnings\n",
    "import copy\n",
    "import os\n",
    "import grid2op\n",
    "import json\n",
    "\n",
    "from grid2op.gym_compat import BoxGymActSpace, BoxGymObsSpace, GymEnv\n",
    "\n",
    "from l2rpn_baselines.PPO_SB3.utils import SB3Agent\n",
    "\n",
    "try:\n",
    "    from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "    from stable_baselines3 import PPO\n",
    "    from stable_baselines3.ppo import MlpPolicy\n",
    "    _CAN_USE_STABLE_BASELINE = True\n",
    "except ImportError:\n",
    "    _CAN_USE_STABLE_BASELINE = False\n",
    "    class MlpPolicy(object):\n",
    "        \"\"\"\n",
    "        Do not use, this class is a template when stable baselines3 is not installed.\n",
    "        \n",
    "        It represents `from stable_baselines3.ppo import MlpPolicy`\n",
    "        \"\"\"\n",
    "    \n",
    "from l2rpn_baselines.PPO_SB3.utils import (default_obs_attr_to_keep, \n",
    "                                           default_act_attr_to_keep,\n",
    "                                           remove_non_usable_attr,\n",
    "                                           save_used_attribute)\n",
    "\n",
    "\n",
    "def get_agent(env,\n",
    "          name=\"PPO_SB3\",\n",
    "          iterations=1,\n",
    "          save_path=None,\n",
    "          load_path=None,\n",
    "          net_arch=None,\n",
    "          logs_dir=None,\n",
    "          learning_rate=3e-4,\n",
    "          save_every_xxx_steps=None,\n",
    "          model_policy=MlpPolicy,\n",
    "          obs_attr_to_keep=copy.deepcopy(default_obs_attr_to_keep),\n",
    "          obs_space_kwargs=None,\n",
    "          act_attr_to_keep=copy.deepcopy(default_act_attr_to_keep),\n",
    "          act_space_kwargs=None,\n",
    "          policy_kwargs=None,\n",
    "          normalize_obs=False,\n",
    "          normalize_act=False,\n",
    "          gymenv_class=GymEnv,\n",
    "          gymenv_kwargs=None,\n",
    "          verbose=True,\n",
    "          seed=None,  # TODO\n",
    "          eval_env=None,  # TODO\n",
    "          **kwargs):\n",
    "    \n",
    "    if not _CAN_USE_STABLE_BASELINE:\n",
    "        raise ImportError(\"Cannot use this function as stable baselines3 is not installed\")\n",
    "    \n",
    "    # keep only usable attributes (if default is used)\n",
    "    act_attr_to_keep = remove_non_usable_attr(env, act_attr_to_keep)\n",
    "    \n",
    "    # save the attributes kept\n",
    "    if save_path is not None:\n",
    "        my_path = os.path.join(save_path, name)\n",
    "    save_used_attribute(save_path, name, obs_attr_to_keep, act_attr_to_keep)\n",
    "\n",
    "    # define the gym environment from the grid2op env\n",
    "    if gymenv_kwargs is None:\n",
    "        gymenv_kwargs = {}\n",
    "    env_gym = gymenv_class(env, **gymenv_kwargs)\n",
    "    env_gym.observation_space.close()\n",
    "    if obs_space_kwargs is None:\n",
    "        obs_space_kwargs = {}\n",
    "    env_gym.observation_space = BoxGymObsSpace(env.observation_space,\n",
    "                                               attr_to_keep=obs_attr_to_keep,\n",
    "                                               **obs_space_kwargs)\n",
    "    env_gym.action_space.close()\n",
    "    if act_space_kwargs is None:\n",
    "        act_space_kwargs = {}\n",
    "    env_gym.action_space = BoxGymActSpace(env.action_space,\n",
    "                                          attr_to_keep=act_attr_to_keep,\n",
    "                                          **act_space_kwargs)\n",
    "\n",
    "    if normalize_act:\n",
    "        if save_path is not None:\n",
    "            with open(os.path.join(my_path, \".normalize_act\"), encoding=\"utf-8\", \n",
    "                      mode=\"w\") as f:\n",
    "                f.write(\"I have encoded the action space !\\n DO NOT MODIFY !\")\n",
    "        for attr_nm in act_attr_to_keep:\n",
    "            if ((\"multiply\" in act_space_kwargs and attr_nm in act_space_kwargs[\"multiply\"]) or \n",
    "                (\"add\" in act_space_kwargs and attr_nm in act_space_kwargs[\"add\"]) \n",
    "               ):\n",
    "                # attribute is scaled elsewhere\n",
    "                continue\n",
    "            env_gym.action_space.normalize_attr(attr_nm)\n",
    "\n",
    "    if normalize_obs:\n",
    "        if save_path is not None:\n",
    "            with open(os.path.join(my_path, \".normalize_obs\"), encoding=\"utf-8\", \n",
    "                      mode=\"w\") as f:\n",
    "                f.write(\"I have encoded the observation space !\\n DO NOT MODIFY !\")\n",
    "        for attr_nm in obs_attr_to_keep:\n",
    "            if ((\"divide\" in obs_space_kwargs and attr_nm in obs_space_kwargs[\"divide\"]) or \n",
    "                (\"subtract\" in obs_space_kwargs and attr_nm in obs_space_kwargs[\"subtract\"]) \n",
    "               ):\n",
    "                # attribute is scaled elsewhere\n",
    "                continue\n",
    "            env_gym.observation_space.normalize_attr(attr_nm)\n",
    "    \n",
    "    # Save a checkpoint every \"save_every_xxx_steps\" steps\n",
    "    checkpoint_callback = None\n",
    "    if save_every_xxx_steps is not None:\n",
    "        if save_path is None:\n",
    "            warnings.warn(\"save_every_xxx_steps is set, but no path are \"\n",
    "                          \"set to save the model (save_path is None). No model \"\n",
    "                          \"will be saved.\")\n",
    "        else:\n",
    "            checkpoint_callback = CheckpointCallback(save_freq=save_every_xxx_steps,\n",
    "                                                     save_path=my_path,\n",
    "                                                     name_prefix=name)\n",
    "\n",
    "    # define the policy\n",
    "    if load_path is None:\n",
    "        if policy_kwargs is None:\n",
    "            policy_kwargs = {}\n",
    "        if net_arch is not None:\n",
    "            policy_kwargs[\"net_arch\"] = net_arch\n",
    "        if logs_dir is not None:\n",
    "            if not os.path.exists(logs_dir):\n",
    "                os.mkdir(logs_dir)\n",
    "            this_logs_dir = os.path.join(logs_dir, name)\n",
    "        else:\n",
    "            this_logs_dir = None\n",
    "                \n",
    "        nn_kwargs = {\n",
    "            \"policy\": model_policy,\n",
    "            \"env\": env_gym,\n",
    "            \"verbose\": verbose,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"tensorboard_log\": this_logs_dir,\n",
    "            \"policy_kwargs\": policy_kwargs,\n",
    "            **kwargs\n",
    "        }\n",
    "        agent = SB3Agent(env.action_space,\n",
    "                         env_gym.action_space,\n",
    "                         env_gym.observation_space,\n",
    "                         nn_kwargs=nn_kwargs,\n",
    "        )\n",
    "    else:        \n",
    "        agent = SB3Agent(env.action_space,\n",
    "                         env_gym.action_space,\n",
    "                         env_gym.observation_space,\n",
    "                         nn_path=os.path.join(load_path, name)\n",
    "        )\n",
    "    return agent, env_gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"l2rpn_wcci_2022_dev\"\n",
    "env_name_train = '_'.join([ENV_NAME, \"train\"])\n",
    "save_path = \"./student\"\n",
    "name = \"CustomGymEnv\"\n",
    "gymenv_class = CustomGymEnv\n",
    "SCOREUSED = ScoreL2RPN2020\n",
    "\n",
    "train_args = {}\n",
    "\n",
    "# Utility parameters PPO\n",
    "train_args[\"logs_dir\"] = \"./logs\"\n",
    "train_args[\"save_path\"] = save_path\n",
    "train_args[\"name\"] = name\n",
    "train_args[\"verbose\"] = 1\n",
    "train_args[\"gymenv_class\"] = gymenv_class\n",
    "train_args[\"device\"] = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_args[\"obs_attr_to_keep\"] = [\"month\", \"day_of_week\", \"hour_of_day\", \"minute_of_hour\",\n",
    "                                  \"gen_p\", \"load_p\", \n",
    "                                  \"p_or\", \"rho\", \"timestep_overflow\", \"line_status\",\n",
    "                                  # dispatch part of the observation\n",
    "                                  \"actual_dispatch\", \"target_dispatch\",\n",
    "                                  # storage part of the observation\n",
    "                                  \"storage_charge\", \"storage_power\",\n",
    "                                  # curtailment part of the observation\n",
    "                                  \"curtailment\", \"curtailment_limit\",  \"gen_p_before_curtail\",\n",
    "                                  ]\n",
    "train_args[\"act_attr_to_keep\"] = [\"set_storage\", \"curtail\"]\n",
    "train_args[\"iterations\"] = 700_000\n",
    "train_args[\"learning_rate\"] = 1e-4\n",
    "train_args[\"net_arch\"] = [300, 300, 300]\n",
    "train_args[\"gamma\"] = 0.999\n",
    "train_args[\"gymenv_kwargs\"] = {\"safe_max_rho\": 0.95}\n",
    "train_args[\"normalize_act\"] = True\n",
    "train_args[\"normalize_obs\"] = True\n",
    "\n",
    "train_args[\"save_every_xxx_steps\"] = min(train_args[\"iterations\"] // 10, 100_000)\n",
    "\n",
    "train_args[\"n_steps\"] = 16\n",
    "train_args[\"batch_size\"] = 16\n",
    "\n",
    "p = Parameters()\n",
    "p.LIMIT_INFEASIBLE_CURTAILMENT_STORAGE_ACTION = True\n",
    "\n",
    "env = grid2op.make(ENV_NAME,\n",
    "                   reward_class=CustomReward,\n",
    "                   backend=LightSimBackend(),\n",
    "                   chronics_class=MultifolderWithCache,\n",
    "                   param=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../preprocess_obs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "  obs_space_kwargs = json.load(f)\n",
    "with open(\"../preprocess_act.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "  act_space_kwargs = json.load(f)\n",
    "\n",
    "student, gym_env = get_agent(env,\n",
    "          obs_space_kwargs=obs_space_kwargs,\n",
    "          act_space_kwargs=act_space_kwargs,\n",
    "          **train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"expert_data.npz\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-train the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class ExpertDataSet(Dataset):\n",
    "  def __init__(self, expert_observations, expert_actions):\n",
    "      self.observations = expert_observations\n",
    "      self.actions = expert_actions\n",
    "      \n",
    "  def __getitem__(self, index):\n",
    "      return (self.observations[index], self.actions[index])\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_acts(actions):\n",
    "  converted_acts = []\n",
    "  for act in actions:\n",
    "    low = gym_env.action_space.dict_properties[\"curtail\"][0]\n",
    "    high = gym_env.action_space.dict_properties[\"curtail\"][1]\n",
    "\n",
    "    curtail = act.curtail.copy()[env.gen_renewable]\n",
    "    curtail = curtail / (high - low) + low\n",
    "\n",
    "    low = gym_env.action_space.dict_properties[\"set_storage\"][0]\n",
    "    high = gym_env.action_space.dict_properties[\"set_storage\"][1]\n",
    "\n",
    "    storage = act.storage_p.copy() / act_space_kwargs[\"multiply\"][\"set_storage\"]\n",
    "\n",
    "    converted_acts.append(np.concatenate((curtail, storage)))\n",
    "  return converted_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_obs(observations):\n",
    "  converted_obs = []\n",
    "  g2op_obs = env.reset()\n",
    "  for obs in observations:\n",
    "    g2op_obs.from_vect(obs)\n",
    "    converted_obs.append(gym_env.observation_space.to_gym(g2op_obs))\n",
    "  return converted_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_observations = convert_obs(data.get(\"expert_observations\"))\n",
    "expert_actions      = convert_acts(data.get(\"expert_actions\"))\n",
    "\n",
    "expert_dataset = ExpertDataSet(expert_observations, expert_actions)\n",
    "\n",
    "train_size = int(0.8 * len(expert_dataset))\n",
    "\n",
    "test_size = len(expert_dataset) - train_size\n",
    "\n",
    "train_expert_dataset, test_expert_dataset = random_split(\n",
    "    expert_dataset, [train_size, test_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_expert_dataset:  21\n",
      "train_expert_dataset:  84\n"
     ]
    }
   ],
   "source": [
    "print(\"test_expert_dataset: \", len(test_expert_dataset))\n",
    "print(\"train_expert_dataset: \", len(train_expert_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_agent(\n",
    "    student,\n",
    "    batch_size=64,\n",
    "    epochs=1000,\n",
    "    learning_rate=1e-4,\n",
    "    log_interval=10,\n",
    "    no_cuda=False,\n",
    "    seed=42,\n",
    "    test_batch_size=64,\n",
    "):\n",
    "  use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "  torch.manual_seed(seed)\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "\n",
    "  criterion = nn.MSELoss()\n",
    "\n",
    "  # Extract initial policy\n",
    "  model = student.nn_model.policy.to(device)\n",
    "\n",
    "  def train(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      action, _, _ = model(data)\n",
    "      action_prediction = action.double()\n",
    "\n",
    "      loss = criterion(action_prediction, target)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      if batch_idx % log_interval == 0:\n",
    "        print(\n",
    "          \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "            epoch,\n",
    "            batch_idx * len(data),\n",
    "            len(train_loader.dataset),\n",
    "            100.0 * batch_idx / len(train_loader),\n",
    "            loss.item(),\n",
    "          )\n",
    "        )\n",
    "\n",
    "  def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        action, _, _ = model(data)\n",
    "        action_prediction = action.double()\n",
    "        test_loss = criterion(action_prediction, target)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Test set: Average loss: {test_loss:.4f}\")\n",
    "\n",
    "  # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
    "  # and testing\n",
    "  train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
    "  )\n",
    "  test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_expert_dataset, batch_size=test_batch_size, shuffle=True, **kwargs,\n",
    "  )\n",
    "\n",
    "  # Define an Optimizer and a learning rate schedule.\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  # Now we are finally ready to train the policy model.\n",
    "  for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer)\n",
    "    test(model, device, test_loader)\n",
    "    optimizer.step()\n",
    "  student.nn_model.policy = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_agent(agent, env_name, nb_scenario, param, verbose):\n",
    "  env_val = grid2op.make(env_name, backend=LightSimBackend(), param=param)\n",
    "  my_score = SCOREUSED(env_val,\n",
    "                        nb_scenario=nb_scenario,\n",
    "                        env_seeds=get_env_seed(env_name)[:nb_scenario],\n",
    "                        agent_seeds=[0 for _ in range(nb_scenario)],\n",
    "                        verbose=verbose,\n",
    "                        nb_process_stats=1)\n",
    "  _, ts_survived, _ = my_score.get(agent)\n",
    "  return np.array(ts_survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval before pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts the evaluation of the agent\n",
      "Start the evaluation of the scores\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47.46666666666667"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = eval_agent(student, \"l2rpn_wcci_2022_dev_val\", 15, p, 1)\n",
    "ts.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/84 (0%)]\tLoss: 569.434649\n",
      "Test set: Average loss: 27.2403\n",
      "Train Epoch: 2 [0/84 (0%)]\tLoss: 569.662933\n",
      "Test set: Average loss: 27.1982\n",
      "Train Epoch: 3 [0/84 (0%)]\tLoss: 569.550270\n",
      "Test set: Average loss: 27.3849\n",
      "Train Epoch: 4 [0/84 (0%)]\tLoss: 570.280076\n",
      "Test set: Average loss: 27.2145\n",
      "Train Epoch: 5 [0/84 (0%)]\tLoss: 569.319070\n",
      "Test set: Average loss: 27.1634\n",
      "Train Epoch: 6 [0/84 (0%)]\tLoss: 568.450434\n",
      "Test set: Average loss: 27.1881\n",
      "Train Epoch: 7 [0/84 (0%)]\tLoss: 568.283108\n",
      "Test set: Average loss: 27.2039\n",
      "Train Epoch: 8 [0/84 (0%)]\tLoss: 567.177313\n",
      "Test set: Average loss: 27.0497\n",
      "Train Epoch: 9 [0/84 (0%)]\tLoss: 568.129128\n",
      "Test set: Average loss: 27.3091\n",
      "Train Epoch: 10 [0/84 (0%)]\tLoss: 566.054794\n",
      "Test set: Average loss: 27.0086\n",
      "Train Epoch: 11 [0/84 (0%)]\tLoss: 567.202367\n",
      "Test set: Average loss: 27.0955\n",
      "Train Epoch: 12 [0/84 (0%)]\tLoss: 566.234754\n",
      "Test set: Average loss: 27.0495\n",
      "Train Epoch: 13 [0/84 (0%)]\tLoss: 565.307330\n",
      "Test set: Average loss: 27.0380\n",
      "Train Epoch: 14 [0/84 (0%)]\tLoss: 566.624153\n",
      "Test set: Average loss: 27.0498\n",
      "Train Epoch: 15 [0/84 (0%)]\tLoss: 565.203602\n",
      "Test set: Average loss: 26.9969\n",
      "Train Epoch: 16 [0/84 (0%)]\tLoss: 564.809596\n",
      "Test set: Average loss: 26.9139\n",
      "Train Epoch: 17 [0/84 (0%)]\tLoss: 564.718327\n",
      "Test set: Average loss: 26.9842\n",
      "Train Epoch: 18 [0/84 (0%)]\tLoss: 563.825403\n",
      "Test set: Average loss: 26.9558\n",
      "Train Epoch: 19 [0/84 (0%)]\tLoss: 563.464108\n",
      "Test set: Average loss: 26.9211\n",
      "Train Epoch: 20 [0/84 (0%)]\tLoss: 563.272398\n",
      "Test set: Average loss: 26.8790\n",
      "Train Epoch: 21 [0/84 (0%)]\tLoss: 562.125181\n",
      "Test set: Average loss: 26.9118\n",
      "Train Epoch: 22 [0/84 (0%)]\tLoss: 563.045323\n",
      "Test set: Average loss: 26.7880\n",
      "Train Epoch: 23 [0/84 (0%)]\tLoss: 561.192807\n",
      "Test set: Average loss: 26.8707\n",
      "Train Epoch: 24 [0/84 (0%)]\tLoss: 563.024911\n",
      "Test set: Average loss: 26.7910\n",
      "Train Epoch: 25 [0/84 (0%)]\tLoss: 561.137303\n",
      "Test set: Average loss: 26.8234\n",
      "Train Epoch: 26 [0/84 (0%)]\tLoss: 561.216065\n",
      "Test set: Average loss: 26.6783\n",
      "Train Epoch: 27 [0/84 (0%)]\tLoss: 560.222761\n",
      "Test set: Average loss: 26.6606\n",
      "Train Epoch: 28 [0/84 (0%)]\tLoss: 559.188641\n",
      "Test set: Average loss: 26.5980\n",
      "Train Epoch: 29 [0/84 (0%)]\tLoss: 560.573995\n",
      "Test set: Average loss: 26.6944\n",
      "Train Epoch: 30 [0/84 (0%)]\tLoss: 559.405393\n",
      "Test set: Average loss: 26.7513\n",
      "Train Epoch: 31 [0/84 (0%)]\tLoss: 559.367698\n",
      "Test set: Average loss: 26.6741\n",
      "Train Epoch: 32 [0/84 (0%)]\tLoss: 558.520517\n",
      "Test set: Average loss: 26.6401\n",
      "Train Epoch: 33 [0/84 (0%)]\tLoss: 557.924263\n",
      "Test set: Average loss: 26.6582\n",
      "Train Epoch: 34 [0/84 (0%)]\tLoss: 558.154789\n",
      "Test set: Average loss: 26.6340\n",
      "Train Epoch: 35 [0/84 (0%)]\tLoss: 557.264268\n",
      "Test set: Average loss: 26.5839\n",
      "Train Epoch: 36 [0/84 (0%)]\tLoss: 556.059646\n",
      "Test set: Average loss: 26.5897\n",
      "Train Epoch: 37 [0/84 (0%)]\tLoss: 556.157699\n",
      "Test set: Average loss: 26.3897\n",
      "Train Epoch: 38 [0/84 (0%)]\tLoss: 556.250434\n",
      "Test set: Average loss: 26.5346\n",
      "Train Epoch: 39 [0/84 (0%)]\tLoss: 555.581213\n",
      "Test set: Average loss: 26.4011\n",
      "Train Epoch: 40 [0/84 (0%)]\tLoss: 555.923396\n",
      "Test set: Average loss: 26.4810\n",
      "Train Epoch: 41 [0/84 (0%)]\tLoss: 555.319046\n",
      "Test set: Average loss: 26.3565\n",
      "Train Epoch: 42 [0/84 (0%)]\tLoss: 555.601285\n",
      "Test set: Average loss: 26.4384\n",
      "Train Epoch: 43 [0/84 (0%)]\tLoss: 554.461516\n",
      "Test set: Average loss: 26.4014\n",
      "Train Epoch: 44 [0/84 (0%)]\tLoss: 554.390057\n",
      "Test set: Average loss: 26.3738\n",
      "Train Epoch: 45 [0/84 (0%)]\tLoss: 553.908195\n",
      "Test set: Average loss: 26.3317\n",
      "Train Epoch: 46 [0/84 (0%)]\tLoss: 554.153831\n",
      "Test set: Average loss: 26.3709\n",
      "Train Epoch: 47 [0/84 (0%)]\tLoss: 552.229239\n",
      "Test set: Average loss: 26.2431\n",
      "Train Epoch: 48 [0/84 (0%)]\tLoss: 552.830222\n",
      "Test set: Average loss: 26.2737\n",
      "Train Epoch: 49 [0/84 (0%)]\tLoss: 552.215598\n",
      "Test set: Average loss: 26.2905\n",
      "Train Epoch: 50 [0/84 (0%)]\tLoss: 550.573272\n",
      "Test set: Average loss: 26.1631\n",
      "Train Epoch: 51 [0/84 (0%)]\tLoss: 551.389662\n",
      "Test set: Average loss: 26.3126\n",
      "Train Epoch: 52 [0/84 (0%)]\tLoss: 551.205119\n",
      "Test set: Average loss: 26.2163\n",
      "Train Epoch: 53 [0/84 (0%)]\tLoss: 550.923881\n",
      "Test set: Average loss: 26.1579\n",
      "Train Epoch: 54 [0/84 (0%)]\tLoss: 549.373939\n",
      "Test set: Average loss: 26.1175\n",
      "Train Epoch: 55 [0/84 (0%)]\tLoss: 550.682220\n",
      "Test set: Average loss: 26.0677\n",
      "Train Epoch: 56 [0/84 (0%)]\tLoss: 549.707803\n",
      "Test set: Average loss: 26.2094\n",
      "Train Epoch: 57 [0/84 (0%)]\tLoss: 551.246047\n",
      "Test set: Average loss: 26.1513\n",
      "Train Epoch: 58 [0/84 (0%)]\tLoss: 549.812193\n",
      "Test set: Average loss: 26.0537\n",
      "Train Epoch: 59 [0/84 (0%)]\tLoss: 548.858833\n",
      "Test set: Average loss: 26.1139\n",
      "Train Epoch: 60 [0/84 (0%)]\tLoss: 548.364512\n",
      "Test set: Average loss: 26.0170\n",
      "Train Epoch: 61 [0/84 (0%)]\tLoss: 547.651385\n",
      "Test set: Average loss: 26.0254\n",
      "Train Epoch: 62 [0/84 (0%)]\tLoss: 547.271627\n",
      "Test set: Average loss: 25.9927\n",
      "Train Epoch: 63 [0/84 (0%)]\tLoss: 547.290401\n",
      "Test set: Average loss: 25.9771\n",
      "Train Epoch: 64 [0/84 (0%)]\tLoss: 545.667716\n",
      "Test set: Average loss: 25.9641\n",
      "Train Epoch: 65 [0/84 (0%)]\tLoss: 548.839013\n",
      "Test set: Average loss: 25.9753\n",
      "Train Epoch: 66 [0/84 (0%)]\tLoss: 546.476870\n",
      "Test set: Average loss: 25.9184\n",
      "Train Epoch: 67 [0/84 (0%)]\tLoss: 544.987372\n",
      "Test set: Average loss: 25.8643\n",
      "Train Epoch: 68 [0/84 (0%)]\tLoss: 545.074742\n",
      "Test set: Average loss: 25.9035\n",
      "Train Epoch: 69 [0/84 (0%)]\tLoss: 545.384016\n",
      "Test set: Average loss: 25.8566\n",
      "Train Epoch: 70 [0/84 (0%)]\tLoss: 543.575439\n",
      "Test set: Average loss: 25.7278\n",
      "Train Epoch: 71 [0/84 (0%)]\tLoss: 544.673605\n",
      "Test set: Average loss: 25.7597\n",
      "Train Epoch: 72 [0/84 (0%)]\tLoss: 544.304800\n",
      "Test set: Average loss: 25.6699\n",
      "Train Epoch: 73 [0/84 (0%)]\tLoss: 544.829242\n",
      "Test set: Average loss: 25.8381\n",
      "Train Epoch: 74 [0/84 (0%)]\tLoss: 542.684408\n",
      "Test set: Average loss: 25.7755\n",
      "Train Epoch: 75 [0/84 (0%)]\tLoss: 542.164525\n",
      "Test set: Average loss: 25.7961\n",
      "Train Epoch: 76 [0/84 (0%)]\tLoss: 542.104614\n",
      "Test set: Average loss: 25.7479\n",
      "Train Epoch: 77 [0/84 (0%)]\tLoss: 542.538267\n",
      "Test set: Average loss: 25.6898\n",
      "Train Epoch: 78 [0/84 (0%)]\tLoss: 542.189397\n",
      "Test set: Average loss: 25.6502\n",
      "Train Epoch: 79 [0/84 (0%)]\tLoss: 541.380965\n",
      "Test set: Average loss: 25.7369\n",
      "Train Epoch: 80 [0/84 (0%)]\tLoss: 540.970796\n",
      "Test set: Average loss: 25.6792\n",
      "Train Epoch: 81 [0/84 (0%)]\tLoss: 540.856794\n",
      "Test set: Average loss: 25.6490\n",
      "Train Epoch: 82 [0/84 (0%)]\tLoss: 541.570150\n",
      "Test set: Average loss: 25.7007\n",
      "Train Epoch: 83 [0/84 (0%)]\tLoss: 540.218211\n",
      "Test set: Average loss: 25.5934\n",
      "Train Epoch: 84 [0/84 (0%)]\tLoss: 539.545404\n",
      "Test set: Average loss: 25.5706\n",
      "Train Epoch: 85 [0/84 (0%)]\tLoss: 539.669089\n",
      "Test set: Average loss: 25.5561\n",
      "Train Epoch: 86 [0/84 (0%)]\tLoss: 539.381234\n",
      "Test set: Average loss: 25.6577\n",
      "Train Epoch: 87 [0/84 (0%)]\tLoss: 538.231078\n",
      "Test set: Average loss: 25.3581\n",
      "Train Epoch: 88 [0/84 (0%)]\tLoss: 537.111082\n",
      "Test set: Average loss: 25.3895\n",
      "Train Epoch: 89 [0/84 (0%)]\tLoss: 538.367318\n",
      "Test set: Average loss: 25.4187\n",
      "Train Epoch: 90 [0/84 (0%)]\tLoss: 538.654563\n",
      "Test set: Average loss: 25.3720\n",
      "Train Epoch: 91 [0/84 (0%)]\tLoss: 536.049128\n",
      "Test set: Average loss: 25.5144\n",
      "Train Epoch: 92 [0/84 (0%)]\tLoss: 536.309212\n",
      "Test set: Average loss: 25.3083\n",
      "Train Epoch: 93 [0/84 (0%)]\tLoss: 536.534005\n",
      "Test set: Average loss: 25.2904\n",
      "Train Epoch: 94 [0/84 (0%)]\tLoss: 535.202990\n",
      "Test set: Average loss: 25.3852\n",
      "Train Epoch: 95 [0/84 (0%)]\tLoss: 535.733389\n",
      "Test set: Average loss: 25.3087\n",
      "Train Epoch: 96 [0/84 (0%)]\tLoss: 535.960637\n",
      "Test set: Average loss: 25.4018\n",
      "Train Epoch: 97 [0/84 (0%)]\tLoss: 534.921374\n",
      "Test set: Average loss: 25.2215\n",
      "Train Epoch: 98 [0/84 (0%)]\tLoss: 535.166913\n",
      "Test set: Average loss: 25.2586\n",
      "Train Epoch: 99 [0/84 (0%)]\tLoss: 535.161850\n",
      "Test set: Average loss: 25.2336\n",
      "Train Epoch: 100 [0/84 (0%)]\tLoss: 533.482799\n",
      "Test set: Average loss: 25.3011\n"
     ]
    }
   ],
   "source": [
    "pretrain_agent(student, batch_size=64, epochs=100, log_interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval after pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts the evaluation of the agent\n",
      "Start the evaluation of the scores\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47.4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = eval_agent(student, \"l2rpn_wcci_2022_dev_val\", 15, p, 1)\n",
    "ts.mean()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25edc466125f4cec8abda921974b846defd6af997a9da267187bfa098e2439de"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('L2RPN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
